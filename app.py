# -*- coding: utf-8 -*-
"""Untitled37.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z0D5bagAnDWPrs5Zq3pAGU9yhd_Y47uA

BASE CODE TO establish the workflow.
"""

import requests
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
model_checkpoint = "aryaumesh/english-to-telugu"

processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to("cuda")

img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg'
raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')

# conditional image captioning prompt, do not touch this.
text = "a photography of"
inputs = processor(raw_image, text, return_tensors="pt").to("cuda")

out = model.generate(**inputs)

inputs = processor(raw_image, return_tensors="pt").to("cuda")

out = model.generate(**inputs)
result = processor.decode(out[0], skip_special_tokens=True)


tokenizer = MBart50TokenizerFast.from_pretrained(model_checkpoint)
model = MBartForConditionalGeneration.from_pretrained(model_checkpoint)
inputs = tokenizer(result, return_tensors="pt")

outputs = model.generate(**inputs)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

# @title Default title text
huggingface_token = "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" # @param {"type":"string","placeholder":"Get your access token from huggingface"}

import gradio as gr
from PIL import Image
import torch
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    MBartForConditionalGeneration,
    MBart50TokenizerFast
)

from huggingface_hub import notebook_login
notebook_login(huggingface_token)

device = "cuda" if torch.cuda.is_available() else "cpu"


blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)


translation_tokenizer = MBart50TokenizerFast.from_pretrained("aryaumesh/english-to-telugu")
translation_model = MBartForConditionalGeneration.from_pretrained("aryaumesh/english-to-telugu").to(device)

def generate_telugu_caption(image: Image.Image) -> str:

    if image.mode != "RGB":
        image = image.convert("RGB")

    #generate tokens in english using blip
    inputs = blip_processor(image, return_tensors="pt").to(device)
    out = blip_model.generate(**inputs)
    english_caption = blip_processor.decode(out[0], skip_special_tokens=True)

    #use bart to yield result in telugu.
    translation_inputs = translation_tokenizer(english_caption, return_tensors="pt").to(device)
    translation_outputs = translation_model.generate(**translation_inputs)
    telugu_caption = translation_tokenizer.decode(translation_outputs[0], skip_special_tokens=True)

    return telugu_caption

# Create the Gradio interface with the given title and description
title = "Major Project - Machine Translation + Image Captioning in Telugu using conditional generation of Blip and BART seq to seq transformer"
description = "Upload or paste an image to generate a Telugu caption."

iface = gr.Interface(
    fn=generate_telugu_caption,
    inputs=gr.Image(type="pil"),
    outputs="text",
    title=title,
    description=description
)

if __name__ == "__main__":
    iface.launch(server_name="0.0.0.0", server_port=7860)

